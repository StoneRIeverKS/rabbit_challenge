{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dse-souken.com/2021/05/18/ai-17/\n",
    "スクリプトは上記から引用した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-10T08:03:07.303621Z",
     "start_time": "2022-01-10T08:03:07.066657Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 5, 8]\n",
      "[[    0.         38337.29360732     0.         31048.8163105\n",
      "      0.             0.             0.             0.\n",
      "      0.        ]\n",
      " [34497.50165261     0.         34498.57122732     0.\n",
      "  42598.01264573     0.             0.             0.\n",
      "      0.        ]\n",
      " [    0.         38338.35793626     0.             0.\n",
      "      0.             0.             0.             0.\n",
      "      0.        ]\n",
      " [34501.13267039     0.             0.             0.\n",
      "      0.             0.         34501.46193262     0.\n",
      "      0.        ]\n",
      " [    0.         38330.40723466     0.             0.\n",
      "      0.         47339.59937017     0.         38336.45869581\n",
      "      0.        ]\n",
      " [    0.             0.             0.             0.\n",
      "  42603.74023701     0.             0.             0.\n",
      "  52603.02024269]\n",
      " [    0.             0.             0.         31047.22739434\n",
      "      0.             0.             0.         38336.98696443\n",
      "      0.        ]\n",
      " [    0.             0.             0.             0.\n",
      "  42599.45504966     0.         34501.38538009     0.\n",
      "      0.        ]\n",
      " [    0.             0.             0.             0.\n",
      "      0.         47342.17960136     0.             0.\n",
      "      0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#必要なモジュールをインポート\n",
    "import numpy as np\n",
    "\n",
    "gamma = 0.9#将来価値の割引。小さいほど行動直後の利益を重視。また、この割引率の存在が効率的な学習の鍵となる。\n",
    "alpha = 0.1#学習率。大きいと1回の学習による値の更新が急激となる。小さいほど更新がゆるやかとなる。後で詳述。\n",
    "\n",
    "#報酬の設定\n",
    "#各場所から移動できる箇所に報酬1を与え、それ以外を0とすることで移動できる方向を指示\n",
    "#以下の行列の各行が場所に対応。0行目は迷路の位置0、1行目が迷路の位置1\n",
    "#ゴールとなる8の報酬を大きく設定する\n",
    "reward = np.array([[0,1,0,1,0,0,0,0,0],#0から1,3に移動できるので、そこに1\n",
    "                   [1,0,1,0,1,0,0,0,0],#1から0,2,4に移動できるので、そこに1\n",
    "                   [0,1,0,0,0,0,0,0,0],\n",
    "                   [1,0,0,0,0,0,1,0,0],\n",
    "                   [0,1,0,0,0,1,0,1,0],\n",
    "                   [0,0,0,0,1,0,0,0,10000],#8をゴールより5→8の報酬を大きく\n",
    "                   [0,0,0,1,0,0,0,1,0],\n",
    "                   [0,0,0,0,1,0,1,0,0],\n",
    "                   [0,0,0,0,0,1,0,0,0]])\n",
    "\n",
    "#Q値(行動価値)の初期値を設定。今回は0を初期値とする。\n",
    "Q = np.array(np.zeros([9,9]))\n",
    "\n",
    "#Q学習を実装し、各位置における行動価値を算出\n",
    "#以下の学習を実行すると、行動価値Qを求められる。Qの各行が位置に対応し、たとえば0行1列目の値は0から1に移動する行動の価値となる。\n",
    "#p_stateのpはpresent(現在)、n_state,n_actionsのnはnext(次)のn\n",
    "for i in range(10000):#1万回繰り返し学習を行う\n",
    "    p_state = np.random.randint(0,9)#現在の状態をランダムに選択\n",
    "    n_actions = []#次の行動の候補を入れる箱\n",
    "    for j in range(9):\n",
    "        if reward[p_state,j] >= 1:#rewardの各行が1以上のインデックスを取得\n",
    "            n_actions.append(j)#これでp_stateの状態で移動できる場所を取得\n",
    "    n_state = np.random.choice(n_actions)#行動可能選択肢からランダムに選択\n",
    "    \n",
    "    #Q値の更新。学習率が小さいほど現在の行動価値が重視され、更新がゆるやかとなる\n",
    "    #ここでQ学習に用いる「たった一つの数式」を利用して行動価値を学習していく\n",
    "    Q[p_state,n_state] = (1-alpha)*Q[p_state,n_state]+alpha*(reward[p_state,n_state]+gamma*Q[n_state,np.argmax(Q[n_state,])])\n",
    "\n",
    "#最短ルート表示関数の定義。Q値が最も高い行動をappendで追加しているだけ\n",
    "def shortest_path(start):#0～8の数字を入力。好きなところからスタート可能\n",
    "    path = [start]#pathに経路を追加していく\n",
    "    p_pos = start#p_posは現在位置(positionの略)\n",
    "    n_pos = p_pos#n_pos(次の位置）にいったんp_posを代入\n",
    "    while(n_pos != 8):#n_posがゴール（8）になるまで繰り返し行動を選択\n",
    "        n_pos = np.argmax(Q[p_pos,])#各位置の行動価値が最も高い行動を選択\n",
    "        path.append(n_pos)#経路をpathに追加\n",
    "        p_pos = n_pos#行動後が次のp_posとなる\n",
    "    return path\n",
    "\n",
    "print(shortest_path(0))#スタートを0として最短経路を表示\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記Qを見ると、最短経路で進むようにQが学習されている。\n",
    "例えば、1個目の要素を見ると\n",
    "[    0.         38337.29360732     0.         31048.8163105\n",
    "      0.             0.             0.             0.\n",
    "      0.        ]\n",
    "となっている。1個目の要素の時点では、エージェントは0の位置におり、1のアクションを取ることで38337、3のアクションを取ることで31048、それ以外のアクションを取ることで0の報酬が得られることがわかる。\n",
    "このとき、エージェントは最大の報酬を得るために動くから1の方向に進む。\n",
    "これを繰り返すことで最短経路を発見することができる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
